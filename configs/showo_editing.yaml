wandb:
  entity: null
  resume: 'auto'

experiment:
  project: "editing"
  name: "show-o-editing-10"
  output_dir: "/data0/arshkon/logs/show-o/showo_editing_10"
  max_train_examples: 20000000
  save_every: 20000
  log_every: 25
  log_grad_norm_every: 500
  resume_from_checkpoint: 'latest'
  evaluation_interval: 250 # Evaluate every 500 steps
  evaluation_steps: 20     # Run evaluation for 20 steps
  visualization_interval: 100 # Run visualization every 500 steps (aligned with eval)
  visualization_samples: 5     # Number of samples to visualize

model:
  vq_model:
    type: "magvitv2"
    vq_model_name: "showlab/magvitv2"
  showo:
    load_from_showo: False
    pretrained_model_path: "showlab/show-o"
    w_clip_vit: False
    vocab_size: 58498
    llm_vocab_size: 50295
    llm_model_path: 'microsoft/phi-1_5'
    codebook_size: 8192
    num_vq_tokens: 256
    num_new_special_tokens: 10  # <|soi|> <|eoi|> <|edit|> <|pad|>

dataset:
  gen_type: "edit"
  params:
    train_metadata_path: "/data0/arshkon/data/edithink/EdiThink/meta/metadata.json"
    dataset_root: "/data0/arshkon/data/edithink/EdiThink/dataset"
    train_split_ratio: 0.9  # Add split ratio
    num_workers: 4
    pin_memory: True
  preprocessing:
    max_seq_length: 1024
    resolution: 256

optimizer:
  name: adamw
  use_8bit_optimizer: True  # Enable 8-bit optimizer for memory efficiency
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.01
    epsilon: 1e-8

lr_scheduler:
  scheduler: "cosine"
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 1000

training:
  gradient_accumulation_steps: 1
  batch_size: 1
  mixed_precision: "bf16"
  enable_tf32: True
  seed: 10086
  max_train_steps: 100000
  cond_dropout_prob: 0.1
  min_masking_rate: 0.0
  label_smoothing: 0.0
  max_grad_norm: 5